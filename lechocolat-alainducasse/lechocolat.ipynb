{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Category Links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Easter chocolates': 'https://www.lechocolat-alainducasse.com/uk/easter-chocolate', 'Boxes': 'https://www.lechocolat-alainducasse.com/uk/chocolates', 'Bars': 'https://www.lechocolat-alainducasse.com/uk/chocolate-bar', 'Gifts': 'https://www.lechocolat-alainducasse.com/uk/chocolate-gift', 'Simple Pleasures': 'https://www.lechocolat-alainducasse.com/uk/simple-pleasures', 'Breakfast & Snacks': 'https://www.lechocolat-alainducasse.com/uk/breakfast-snacks'}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.lechocolat-alainducasse.com/uk/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    menu_items = soup.select('.siteMenuItem')\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for item in menu_items:\n",
    "        try:\n",
    "            link = item.select_one('a')['href']\n",
    "            content = item.select_one('a span').text.strip()\n",
    "            data[content] = link\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(data)\n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping PDP's Links in each Cateogry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_product_urls(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_links = soup.select(\"#js-product-list .productMiniature a\")\n",
    "\n",
    "        urls = [link['href'] for link in product_links]\n",
    "\n",
    "        return urls\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# url = \"https://www.lechocolat-alainducasse.com/uk/easter-chocolate\"\n",
    "# product_urls = extract_product_urls(url)\n",
    "# print(product_urls)\n",
    "\n",
    "pdp_dict={}\n",
    "for data_value in data:\n",
    "    pdp_links=extract_product_urls(data[data_value])\n",
    "    pdp_dict[data_value]=pdp_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_product(token, id_product, id_customization, group_value, quantity):\n",
    "    headers = {\n",
    "        'accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "        'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "        'x-requested-with': 'XMLHttpRequest',\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'controller': 'product',\n",
    "        'token': token,\n",
    "        'id_product': id_product,\n",
    "        'id_customization': id_customization,\n",
    "        'group[6]': group_value,\n",
    "        'qty': quantity,\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'quickview': '0',\n",
    "        'ajax': '1',\n",
    "        'action': 'refresh',\n",
    "        'quantity_wanted': quantity,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        'https://www.lechocolat-alainducasse.com/uk/index.php',\n",
    "        params=params,\n",
    "        headers=headers,\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_data(soup):\n",
    "    form = soup.find('form', id='add-to-cart-or-refresh')\n",
    "\n",
    "    if form:\n",
    "        input_fields = form.find_all('input')\n",
    "\n",
    "        params = {}\n",
    "        for field in input_fields:\n",
    "            name = field.get('name')\n",
    "            value = field.get('value')\n",
    "            if name and value:\n",
    "                params[name] = value\n",
    "\n",
    "        token = params.get('token', '')\n",
    "        id_product = params.get('id_product', '')\n",
    "        id_customization = params.get('id_customization', '')\n",
    "        group_value = params.get('group[6]', '')\n",
    "        quantity = params.get('qty', '')\n",
    "\n",
    "        response_text = refresh_product(token, id_product, id_customization, group_value, quantity)\n",
    "\n",
    "        json_data = json.loads(response_text)\n",
    "        product_details = json_data.get('product_details', '').strip()\n",
    "        soup = BeautifulSoup(product_details, 'html.parser')\n",
    "        product_details_div = soup.select('#product-details')\n",
    "\n",
    "        if product_details_div:\n",
    "            data_product_value = product_details_div[0].get('data-product')\n",
    "            varaint_data = json.loads(data_product_value)\n",
    "\n",
    "            price = varaint_data.get('price', '')\n",
    "            description = varaint_data.get('meta_description', '')\n",
    "            title = varaint_data.get('meta_title', '')\n",
    "            link = varaint_data.get('link', '')\n",
    "\n",
    "            features_dict = {}\n",
    "            for features_data in varaint_data.get('features', []):\n",
    "                features_dict[features_data.get('name', '')] = features_data.get('value', '')\n",
    "\n",
    "            availability_message = varaint_data.get('availability_message', '')\n",
    "\n",
    "            image_list = []\n",
    "            for image_dict in varaint_data.get('images', [])[0].get('bySize', []):\n",
    "                image_link = varaint_data.get('images', [])[0].get('bySize', {}).get(image_dict, {}).get('url', '')\n",
    "                image_list.append(image_link)\n",
    "\n",
    "            image_list = list(set(image_list))\n",
    "\n",
    "            unit = varaint_data.get('attributes', {}).get('6', {}).get('name', '')\n",
    "            price_float = float(re.search(r'\\d+\\.\\d+', price).group())\n",
    "\n",
    "            product_data = {\n",
    "                'title': title,\n",
    "                'selling_price': price_float,\n",
    "                'unit': unit,\n",
    "                'availability': availability_message,\n",
    "                'description': description,\n",
    "                'link': link,\n",
    "                'features': features_dict,\n",
    "                'images': image_list\n",
    "            }\n",
    "            return product_data\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_variant_product_info(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    product_info = {}\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "        title = soup.title.get_text()\n",
    "        product_info['title'] = title\n",
    "\n",
    "        description_element = soup.select_one('#product_tab_informations p:nth-child(1)')\n",
    "        description = description_element.get_text(strip=True) if description_element else None\n",
    "        product_info['description'] = description        \n",
    "\n",
    "        # Extracting breadcrumb\n",
    "        breadcrumb_elements = soup.select('.breadcrumb li span')\n",
    "        breadcrumb_list = [element.get_text(strip=True) for element in breadcrumb_elements]\n",
    "        product_info['breadcrumb'] = breadcrumb_list\n",
    "\n",
    "        selling_price_tag = soup.find(\"meta\", property=\"product:price:amount\")\n",
    "        selling_price_value = selling_price_tag[\"content\"] if selling_price_tag else None\n",
    "        \n",
    "        selling_price_currency_tag = soup.find(\"meta\", property=\"product:price:currency\")\n",
    "        selling_price_currency_value = selling_price_currency_tag[\"content\"] if selling_price_currency_tag else None\n",
    "        \n",
    "        # if selling_price_value and selling_price_currency_value == \"GBP\":\n",
    "        #     selling_price = \"£\" + selling_price_value\n",
    "        # else:\n",
    "        #     selling_price = None\n",
    "        try:\n",
    "            product_info['selling_price'] = float(selling_price_value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        weight_element = soup.find(class_=\"productCard__weight\")\n",
    "        \n",
    "        if weight_element:\n",
    "            weight_text = weight_element.get_text(strip=True)\n",
    "            \n",
    "            unit = weight_text.split()[-1]\n",
    "            \n",
    "            product_info['unit'] = unit\n",
    "        else:\n",
    "            product_info['unit'] = None \n",
    "\n",
    "        message_tag = soup.find('p', class_='mailAlert__message')\n",
    "\n",
    "        if message_tag and \"This product is unavailable\" in message_tag.get_text():\n",
    "            availability = \"Out of Stock\"\n",
    "        else:\n",
    "            availability = \"In Stock\"\n",
    "        product_info['availability'] = availability                      \n",
    "\n",
    "        og_image_tag = soup.find(\"meta\", property=\"og:image\")\n",
    "        og_image_url = og_image_tag[\"content\"] if og_image_tag else None\n",
    "        product_info['image'] = og_image_url\n",
    "        \n",
    "        # Extracting image URLs\n",
    "        image_links = soup.select('.productImages__list li a')\n",
    "        image_urls = [link['href'] for link in image_links]\n",
    "        product_info['images'] = image_urls\n",
    "        \n",
    "        key_value_pairs = {}\n",
    "        elements = soup.select('.wysiwyg-title-default')\n",
    "        for element in elements:\n",
    "            key = element.get_text(strip=True)\n",
    "            next_p = element.find_next_sibling('p')\n",
    "            value = next_p.get_text(strip=True) if next_p else None\n",
    "            key_value_pairs[key] = value\n",
    "        product_info['features'] = key_value_pairs\n",
    "\n",
    "        product_info['url'] = url       \n",
    "        \n",
    "        ordered_keys = [\n",
    "            'title',\n",
    "            'description',\n",
    "            'breadcrumb',\n",
    "            'selling_price',\n",
    "            'unit',\n",
    "            'availability',\n",
    "            'image',\n",
    "            'images',\n",
    "            'features',\n",
    "            'url'\n",
    "        ]\n",
    "\n",
    "        ordered_product_info = {key: product_info[key] for key in ordered_keys if key in product_info}\n",
    "\n",
    "        return ordered_product_info\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "        print(url)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def scrape_product_info(url,category):\n",
    "    response = requests.get(url)\n",
    "    product_info = {}\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        title = soup.title.get_text()\n",
    "        product_info['title'] = title  \n",
    "\n",
    "        description_element = soup.select_one('#product_tab_informations p:nth-child(1)')\n",
    "        description = description_element.get_text(strip=True) if description_element else None\n",
    "        product_info['description'] = description                        \n",
    "\n",
    "        breadcrumb_elements = soup.select('.breadcrumb li span')\n",
    "        breadcrumb_list = [element.get_text(strip=True) for element in breadcrumb_elements]\n",
    "        product_info['breadcrumb'] = breadcrumb_list\n",
    "\n",
    "        image_links = soup.select('.productImages__list li a')\n",
    "        image_urls = [link['href'] for link in image_links]\n",
    "        product_info['images'] = image_urls\n",
    "\n",
    "        key_value_pairs = {}\n",
    "        elements = soup.select('.wysiwyg-title-default')\n",
    "        for element in elements:\n",
    "            key = element.get_text(strip=True)\n",
    "            next_p = element.find_next_sibling('p')\n",
    "            value = next_p.get_text(strip=True) if next_p else None\n",
    "            key_value_pairs[key] = value\n",
    "        product_info['features'] = key_value_pairs\n",
    "\n",
    "        og_image_tag = soup.find(\"meta\", property=\"og:image\")\n",
    "        og_image_url = og_image_tag[\"content\"] if og_image_tag else None\n",
    "        product_info['image'] = og_image_url\n",
    "\n",
    "        selling_price_tag = soup.find(\"meta\", property=\"product:price:amount\")\n",
    "        selling_price_value = selling_price_tag[\"content\"] if selling_price_tag else None\n",
    "\n",
    "        selling_price_currency_tag = soup.find(\"meta\", property=\"product:price:currency\")\n",
    "        selling_price_currency_value = selling_price_currency_tag[\"content\"] if selling_price_currency_tag else None\n",
    "\n",
    "        # if selling_price_value and selling_price_currency_value == \"GBP\":\n",
    "        #     selling_price = \"£\" + selling_price_value\n",
    "        # else:\n",
    "        #     selling_price = None\n",
    "        try:\n",
    "            product_info['selling_price'] = float(selling_price_value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        weight_element = soup.find(class_=\"productCard__weight\")\n",
    "        if weight_element:\n",
    "            weight_text = weight_element.get_text(strip=True)\n",
    "            unit = weight_text.split()[-1]\n",
    "            product_info['unit'] = unit\n",
    "        else:\n",
    "            product_info['unit'] = None\n",
    "\n",
    "        product_info['url'] = url\n",
    "        \n",
    "        variant_products = soup.select('.linkedProducts__list li a')\n",
    "        varinat_list=[]\n",
    "        if variant_products:\n",
    "            for product in variant_products:\n",
    "                link = product['href']\n",
    "                variant_response=scrape_variant_product_info(link)\n",
    "                varinat_list.append(variant_response)\n",
    "        try:\n",
    "            varaint_json=get_product_data(soup)\n",
    "            varinat_list.append(varaint_json)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        product_info['variants'] = varinat_list\n",
    "\n",
    "        message_tag = soup.find('p', class_='mailAlert__message')\n",
    "        if message_tag and \"This product is unavailable\" in message_tag.get_text():\n",
    "            availability = \"Out of Stock\"\n",
    "        else:\n",
    "            availability = \"In Stock\"\n",
    "        product_info['availability'] = availability\n",
    "        product_info['category'] = category\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "        print(url)\n",
    "        return None\n",
    "\n",
    "    ordered_keys = [\n",
    "        'title',\n",
    "        'description',\n",
    "        'breadcrumb',\n",
    "        'images',\n",
    "        'features',\n",
    "        'image',\n",
    "        'selling_price',\n",
    "        'unit',\n",
    "        'url',\n",
    "        'variants',\n",
    "        'availability',\n",
    "        'category'\n",
    "    ]\n",
    "\n",
    "    ordered_product_info = {key: product_info[key] for key in ordered_keys if key in product_info}\n",
    "\n",
    "    return ordered_product_info\n",
    "\n",
    "\n",
    "url = \"https://www.lechocolat-alainducasse.com/uk/easter-treats-milk#/77-size-150g\"\n",
    "category=\"XYZ\"\n",
    "product_data = scrape_product_info(url,category)\n",
    "print(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easter chocolates\n",
      "Boxes\n",
      "Failed to retrieve the page. Status code: 404\n",
      "https://www.lechocolat-alainducasse.com/uk/coffret-carres-degustation-50-pieces\n",
      "Bars\n",
      "Failed to retrieve the page. Status code: 404\n",
      "https://www.lechocolat-alainducasse.com/uk/coffret-carres-degustation-50-pieces\n",
      "Gifts\n",
      "Simple Pleasures\n",
      "Failed to retrieve the page. Status code: 404\n",
      "https://www.lechocolat-alainducasse.com/uk/coffret-carres-degustation-50-pieces\n",
      "Breakfast & Snacks\n",
      "Failed to retrieve the page. Status code: 404\n",
      "https://www.lechocolat-alainducasse.com/uk/coffret-carres-degustation-50-pieces\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "pdp_json_dict = []\n",
    "\n",
    "for category_value in pdp_dict:\n",
    "    print(category_value)\n",
    "    for pdp_links in pdp_dict[category_value]:\n",
    "        try:\n",
    "            url = pdp_links\n",
    "            category = category_value\n",
    "            product_data = scrape_product_info(url, category)\n",
    "            if product_data is not None:\n",
    "                pdp_json_dict.append(product_data)\n",
    "            time.sleep(random.randint(1, 3))\n",
    "        except Exception as ex:\n",
    "            print(pdp_links)\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Output\\Lechocolat.json', 'w') as json_file:\n",
    "    json.dump(pdp_json_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
